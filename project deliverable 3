import os
import sys
import subprocess
import signal
import time
import threading
import heapq
import collections
import random # For simulating page access and producer-consumer

# ----------------- Global Configuration for Paging ----------------- #
PAGE_SIZE = 4  # Size of each page frame (e.g., 4 units/KB)
MAX_MEMORY_PAGES = 10  # Total number of page frames available in memory
memory = [(None, None) for _ in range(MAX_MEMORY_PAGES)]  # Represents physical memory: (process_id, virtual_page_number)
memory_lock = threading.Lock() # Lock for protecting memory access

process_memory_map = collections.defaultdict(list) # Maps process_id to a list of (physical_frame_index, virtual_page_number) tuples
page_fault_counts = collections.defaultdict(int) # Tracks page faults per process

# For FIFO page replacement
fifo_queue = collections.deque() # Stores (physical_frame_index, process_id, virtual_page_number) in order of loading

# For LRU page replacement
lru_tracker = {} # Stores last access time for each physical frame: {physical_frame_index: timestamp}
lru_lock = threading.Lock() # Lock for LRU tracker updates

# ----------------- Global Configuration for Synchronization ----------------- #
shared_resource_lock = threading.Lock() # Mutex for general shared resource access

# Producer-Consumer problem setup
BUFFER_SIZE = 5
shared_buffer = collections.deque(maxlen=BUFFER_SIZE)
buffer_lock = threading.Lock()
buffer_full = threading.Semaphore(0) # Initially no items in buffer
buffer_empty = threading.Semaphore(BUFFER_SIZE) # Initially buffer has BUFFER_SIZE empty slots

# ----------------- Job Tracking ----------------- #
jobs = {}
job_counter = 1

class Job:
    def __init__(self, job_id, process, command, priority=0, memory_pages=0):
        self.job_id = job_id
        self.process = process
        self.command = command
        self.priority = priority
        self.status = "Running"
        self.arrival_time = time.time()
        self.start_time = None
        self.finish_time = None
        self.memory_pages = memory_pages # Number of pages requested by this job

    def waiting_time(self):
        if self.start_time is not None:
            return round(self.start_time - self.arrival_time, 2)
        return None

    def turnaround_time(self):
        if self.finish_time is not None:
            return round(self.finish_time - self.arrival_time, 2)
        return None

    def response_time(self):
        if self.start_time is not None:
            return round(self.start_time - self.arrival_time, 2)
        return None

# ----------------- Memory Management Functions ----------------- #

def _find_free_page_frame():
    """Finds an empty page frame in memory."""
    with memory_lock:
        for i, (pid, vpn) in enumerate(memory):
            if pid is None:
                return i
    return -1 # No free frame found

def _replace_page_fifo(process_id, virtual_page_to_load):
    """
    Implements the FIFO page replacement algorithm.
    Removes the oldest page from memory and loads the new one.
    """
    global memory, fifo_queue, process_memory_map, lru_tracker

    if not fifo_queue:
        print("FIFO queue is empty, cannot replace page.")
        return False

    with memory_lock:
        frame_to_replace_idx, old_pid, old_vpn = fifo_queue.popleft()

        print(f"Memory Full! FIFO: Replacing page ({old_pid}, {old_vpn}) from frame {frame_to_replace_idx} to load ({process_id}, {virtual_page_to_load}).")

        # Remove the old page's entry from its process's memory map
        if (frame_to_replace_idx, old_vpn) in process_memory_map[old_pid]:
            process_memory_map[old_pid].remove((frame_to_replace_idx, old_vpn))

        # Update memory
        memory[frame_to_replace_idx] = (process_id, virtual_page_to_load)

        # Add the new page to the FIFO queue
        fifo_queue.append((frame_to_replace_idx, process_id, virtual_page_to_load))

        # Update LRU tracker if it was tracking the replaced page (not strictly necessary for FIFO, but good for consistency)
        with lru_lock:
            if frame_to_replace_idx in lru_tracker:
                del lru_tracker[frame_to_replace_idx]
            lru_tracker[frame_to_replace_idx] = time.time()

        return frame_to_replace_idx

def _replace_page_lru(process_id, virtual_page_to_load):
    """
    Implements the LRU page replacement algorithm.
    Removes the least recently used page from memory and loads the new one.
    """
    global memory, lru_tracker, process_memory_map, fifo_queue

    if not lru_tracker:
        print("LRU tracker is empty, cannot replace page.")
        return False

    with memory_lock:
        with lru_lock:
            # Find the least recently used page frame
            least_recent_frame_idx = min(lru_tracker, key=lru_tracker.get)
            old_pid, old_vpn = memory[least_recent_frame_idx]

            print(f"Memory Full! LRU: Replacing page ({old_pid}, {old_vpn}) from frame {least_recent_frame_idx} to load ({process_id}, {virtual_page_to_load}).")

            # Remove the old page's entry from its process's memory map
            if (least_recent_frame_idx, old_vpn) in process_memory_map[old_pid]:
                process_memory_map[old_pid].remove((least_recent_frame_idx, old_vpn))

            # Update memory
            memory[least_recent_frame_idx] = (process_id, virtual_page_to_load)

            # Update LRU tracker for the new page
            lru_tracker[least_recent_frame_idx] = time.time()

            # Remove the old page from FIFO queue if it exists (for consistency if both algorithms are used)
            if (least_recent_frame_idx, old_pid, old_vpn) in fifo_queue:
                fifo_queue.remove((least_recent_frame_idx, old_pid, old_vpn))
            # Add the new page to the FIFO queue (even if not FIFO replacement, for consistency)
            fifo_queue.append((least_recent_frame_idx, process_id, virtual_page_to_load))

        return least_recent_frame_idx

def allocate_pages(process_id, num_pages):
    """
    Allocates a specified number of pages for a process.
    Attempts to use free frames first, then triggers page replacement if memory is full.
    """
    global memory, process_memory_map, fifo_queue, lru_tracker

    allocated_pages = 0
    pages_to_allocate = [] # Store virtual page numbers that need allocation
    for i in range(num_pages):
        pages_to_allocate.append(i) # Assuming virtual pages 0 to num_pages-1 for simplicity

    for virtual_page_number in pages_to_allocate:
        frame_idx = _find_free_page_frame()
        if frame_idx != -1:
            with memory_lock:
                memory[frame_idx] = (process_id, virtual_page_number)
                process_memory_map[process_id].append((frame_idx, virtual_page_number))
                fifo_queue.append((frame_idx, process_id, virtual_page_number))
                with lru_lock:
                    lru_tracker[frame_idx] = time.time()
                allocated_pages += 1
                print(f"Process {process_id}: Allocated page {virtual_page_number} to frame {frame_idx}.")
        else:
            print(f"Memory full for process {process_id} requiring {num_pages - allocated_pages} more pages. Attempting page replacement...")
            # Here, we can choose a default replacement algorithm or make it configurable
            # For simplicity, let's always try LRU for initial allocation if full
            replaced_frame_idx = _replace_page_lru(process_id, virtual_page_number) # Or _replace_page_fifo
            if replaced_frame_idx is not False:
                process_memory_map[process_id].append((replaced_frame_idx, virtual_page_number))
                allocated_pages += 1
            else:
                print(f"Could not allocate page {virtual_page_number} for process {process_id}. No replacement candidate found.")
                break # Stop if replacement failed

    return allocated_pages

def deallocate_pages(process_id):
    """Deallocates all pages associated with a process."""
    global memory, process_memory_map, fifo_queue, lru_tracker
    with memory_lock:
        if process_id in process_memory_map:
            for frame_idx, virtual_page_number in process_memory_map[process_id]:
                memory[frame_idx] = (None, None)
                # Remove from FIFO queue
                if (frame_idx, process_id, virtual_page_number) in fifo_queue:
                    fifo_queue.remove((frame_idx, process_id, virtual_page_number))
                # Remove from LRU tracker
                with lru_lock:
                    if frame_idx in lru_tracker:
                        del lru_tracker[frame_idx]
            print(f"Process {process_id}: Deallocated all pages.")
            del process_memory_map[process_id]
        else:
            print(f"Process {process_id} has no allocated memory.")

def access_page(process_id, virtual_page_number, algorithm='lru'):
    """
    Simulates a process accessing a page. Handles page faults and replacement.
    """
    global memory, process_memory_map, page_fault_counts, fifo_queue, lru_tracker

    page_found = False
    physical_frame_idx = -1

    with memory_lock:
        for frame_idx, (pid, vpn) in enumerate(memory):
            if pid == process_id and vpn == virtual_page_number:
                page_found = True
                physical_frame_idx = frame_idx
                break

    if page_found:
        print(f"Process {process_id}: Page {virtual_page_number} found in memory at frame {physical_frame_idx}.")
        with lru_lock:
            lru_tracker[physical_frame_idx] = time.time() # Update access time for LRU
    else:
        page_fault_counts[process_id] += 1
        print(f"Process {process_id}: PAGE FAULT for page {virtual_page_number}! Count: {page_fault_counts[process_id]}")

        frame_idx = _find_free_page_frame()
        if frame_idx != -1:
            with memory_lock:
                memory[frame_idx] = (process_id, virtual_page_number)
                process_memory_map[process_id].append((frame_idx, virtual_page_number))
                fifo_queue.append((frame_idx, process_id, virtual_page_number))
                with lru_lock:
                    lru_tracker[frame_idx] = time.time()
            print(f"Process {process_id}: Loaded page {virtual_page_number} into free frame {frame_idx}.")
        else:
            print("Memory is full. Initiating page replacement...")
            replaced_frame_idx = -1
            if algorithm == 'fifo':
                replaced_frame_idx = _replace_page_fifo(process_id, virtual_page_number)
            elif algorithm == 'lru':
                replaced_frame_idx = _replace_page_lru(process_id, virtual_page_number)
            else:
                print("Invalid page replacement algorithm specified.")
                return False

            if replaced_frame_idx is not False:
                # Add the new page to the process's memory map if it's not already there (could be if it was replaced)
                # Ensure the entry is updated if it was an existing page but replaced
                if (replaced_frame_idx, virtual_page_number) not in process_memory_map[process_id]:
                    process_memory_map[process_id].append((replaced_frame_idx, virtual_page_number))
                print(f"Process {process_id}: Page {virtual_page_number} loaded into frame {replaced_frame_idx} via {algorithm.upper()} replacement.")
            else:
                print(f"Process {process_id}: Failed to load page {virtual_page_number} even after replacement attempt.")
                return False
    return True

# ----------------- Synchronization Problem: Producer-Consumer ----------------- #

def producer(items_to_produce):
    """
    Producer function: Adds items to the shared buffer.
    """
    for i in range(items_to_produce):
        item = f"item_{i}"
        buffer_empty.acquire() # Wait if buffer is full
        with buffer_lock:
            shared_buffer.append(item)
            print(f"Producer: Produced {item}. Buffer: {list(shared_buffer)}")
        buffer_full.release() # Signal that buffer has an item
        time.sleep(random.uniform(0.1, 0.5))

def consumer(items_to_consume):
    """
    Consumer function: Removes items from the shared buffer.
    """
    for _ in range(items_to_consume):
        buffer_full.acquire() # Wait if buffer is empty
        with buffer_lock:
            item = shared_buffer.popleft()
            print(f"Consumer: Consumed {item}. Buffer: {list(shared_buffer)}")
        buffer_empty.release() # Signal that buffer has an empty slot
        time.sleep(random.uniform(0.1, 0.7))

# ----------------- Built-in Commands ----------------- #
def cd(args):
    try:
        os.chdir(args[1])
    except IndexError:
        print("cd: missing operand")
    except FileNotFoundError:
        print("cd: no such directory")

def pwd(args):
    print(os.getcwd())

def echo(args):
    print(" ".join(args[1:]))

def clear(args):
    os.system("clear")

def ls(args):
    print("\n".join(os.listdir(os.getcwd())))

def cat(args):
    try:
        with open(args[1], 'r') as f:
            print(f.read())
    except IndexError:
        print("cat: missing filename")
    except Exception as e:
        print(f"cat: {e}")

def mkdir(args):
    try:
        os.mkdir(args[1])
    except IndexError:
        print("mkdir: missing directory name")
    except Exception as e:
        print(f"mkdir: {e}")

def rmdir(args):
    try:
        os.rmdir(args[1])
    except IndexError:
        print("rmdir: missing directory name")
    except Exception as e:
        print(f"rmdir: {e}")

def rm(args):
    try:
        os.remove(args[1])
    except IndexError:
        print("rm: missing filename")
    except Exception as e:
        print(f"rm: {e}")

def touch(args):
    try:
        with open(args[1], 'a'):
            os.utime(args[1], None)
    except IndexError:
        print("touch: missing filename")
    except Exception as e:
        print(f"touch: {e}")

def kill_proc(args):
    try:
        pid = int(args[1])
        os.kill(pid, signal.SIGTERM)
        # Also deallocate memory when a process is killed
        deallocate_pages(pid)
    except IndexError:
        print("kill: missing PID")
    except Exception as e:
        print(f"kill: {e}")

def exit_shell(args):
    print("Exiting shell.")
    os._exit(0)

def jobs_cmd(args):
    for jid, job in jobs.items():
        status = job.status if job.process.poll() is None else "Done"
        print(f"[{jid}] {job.process.pid} {status} {job.command}")

def fg(args):
    try:
        jid = int(args[1])
        job = jobs.pop(jid)
        if job.process.poll() is None:
            print(f"Bringing job [{jid}] {job.command} to foreground")
            os.killpg(os.getpgid(job.process.pid), signal.SIGCONT)
            job.process.wait()
        else:
            print(f"Job [{jid}] has already completed")
            deallocate_pages(job.process.pid) # Deallocate memory if job completed
    except KeyError:
        print(f"fg: no such job [{args[1]}]")
    except Exception as e:
        print(f"fg: {e}")

def bg(args):
    try:
        jid = int(args[1])
        job = jobs[jid]
        if job.process.poll() is None:
            os.killpg(os.getpgid(job.process.pid), signal.SIGCONT)
            print(f"[{jid}] {job.process.pid} resumed in background")
        else:
            print(f"Job [{jid}] is already done")
            deallocate_pages(job.process.pid) # Deallocate memory if job completed
    except KeyError:
        print(f"bg: no such job [{args[1]}]")
    except Exception as e:
        print(f"bg: {e}")

# New built-in: Memory Information
def meminfo(args):
    print("\n--- Memory Status ---")
    print(f"Page Size: {PAGE_SIZE} units")
    print(f"Max Memory Pages: {MAX_MEMORY_PAGES}")
    print("\nPhysical Memory Layout (Frame: (PID, VPN))")
    for i, (pid, vpn) in enumerate(memory):
        print(f"  Frame {i:2}: {'Free' if pid is None else f'({pid}, {vpn})'}")

    print("\nProcess Memory Usage:")
    total_used_memory = 0
    for pid, pages in process_memory_map.items():
        if pid in jobs and jobs[pid].process.poll() is not None: # Clean up if process is done
            deallocate_pages(pid)
            continue
        mem_used = len(pages) * PAGE_SIZE
        total_used_memory += mem_used
        print(f"  Process {pid}: {len(pages)} pages ({mem_used} units)")
    print(f"\nTotal Memory Used: {total_used_memory} units / {MAX_MEMORY_PAGES * PAGE_SIZE} units")

    print("\nPage Fault Counts:")
    for pid, faults in page_fault_counts.items():
        if pid in jobs and jobs[pid].process.poll() is not None: # Clean up if process is done
            continue
        print(f"  Process {pid}: {faults} faults")
    print("---------------------\n")

# New built-in: Access Page
def accesspage(args):
    try:
        process_id = int(args[1])
        virtual_page_number = int(args[2])
        algorithm = args[3].lower() if len(args) > 3 else 'lru'
        if algorithm not in ['fifo', 'lru']:
            print("Invalid algorithm. Choose 'fifo' or 'lru'. Defaulting to LRU.")
            algorithm = 'lru'

        # Ensure the process exists and is running
        if process_id not in jobs or jobs[process_id].process.poll() is not None:
            print(f"Error: Process {process_id} not found or not running.")
            return

        access_page(process_id, virtual_page_number, algorithm)

    except IndexError:
        print("Usage: accesspage <process_id> <virtual_page_number> [fifo|lru]")
    except ValueError:
        print("Invalid process ID or page number.")
    except Exception as e:
        print(f"Error accessing page: {e}")

# New built-in: Run Producer-Consumer demonstration
def run_producer_consumer(args):
    try:
        num_producers = int(args[1])
        num_consumers = int(args[2])
        items_per_producer = int(args[3])
        items_per_consumer = int(args[4])

        producer_threads = []
        consumer_threads = []

        print(f"\n--- Starting Producer-Consumer Simulation (Buffer Size: {BUFFER_SIZE}) ---")

        for i in range(num_producers):
            p_thread = threading.Thread(target=producer, args=(items_per_producer,), name=f"Producer-{i+1}")
            producer_threads.append(p_thread)
            p_thread.start()

        for i in range(num_consumers):
            c_thread = threading.Thread(target=consumer, args=(items_per_consumer,), name=f"Consumer-{i+1}")
            consumer_threads.append(c_thread)
            c_thread.start()

        # Wait for all producer and consumer threads to finish
        for t in producer_threads:
            t.join()
        for t in consumer_threads:
            t.join()

        print("--- Producer-Consumer Simulation Finished ---\n")

    except IndexError:
        print("Usage: run_producer_consumer <num_producers> <num_consumers> <items_per_producer> <items_per_consumer>")
    except ValueError:
        print("Invalid number for arguments.")
    except Exception as e:
        print(f"Error in Producer-Consumer simulation: {e}")

# ----------------- Scheduling Infrastructure ----------------- #
SCHEDULERS = {'rr': 'Round-Robin', 'pbs': 'Priority-Based'}
current_scheduler = None
quantum = 1
scheduler_thread = None
scheduler_event = threading.Event()
rr_queue = []
pbs_queue = []
queue_lock = threading.Lock()

def round_robin_scheduler():
    global rr_queue, quantum
    while not scheduler_event.is_set():
        with queue_lock:
            if not rr_queue:
                time.sleep(0.1)
                continue
            current_job = rr_queue.pop(0)
            if current_job.start_time is None:
                current_job.start_time = time.time()
            current_job.status = "Running"
        print(f"\n[RR] Running job {current_job.job_id} for {quantum}s: {current_job.command}")
        start_time = time.time()
        while time.time() - start_time < quantum:
            if scheduler_event.is_set():
                return
            time.sleep(0.1)
            if current_job.process.poll() is not None:
                # If process finished, deallocate its memory
                deallocate_pages(current_job.process.pid)
                break
        with queue_lock:
            if current_job.process.poll() is None:
                current_job.status = "Waiting"
                rr_queue.append(current_job)
                print(f"[RR] Time slice expired for job {current_job.job_id}")
            else:
                current_job.finish_time = time.time()
                print(f"[RR] Job {current_job.job_id} completed")
                if current_job.job_id in jobs:
                    jobs[current_job.job_id].status = "Done"
                deallocate_pages(current_job.process.pid) # Deallocate memory after completion

def priority_scheduler():
    global pbs_queue
    current_job = None
    while not scheduler_event.is_set():
        with queue_lock:
            if not pbs_queue:
                time.sleep(0.1)
                continue
            _, _, next_job = heapq.heappop(pbs_queue)
            if next_job.start_time is None:
                next_job.start_time = time.time()
            if current_job is not None and next_job.priority < current_job.priority:
                current_job.status = "Waiting"
                heapq.heappush(pbs_queue, (current_job.priority, current_job.arrival_time, current_job))
                print(f"\n[PBS] Preempting job {current_job.job_id}")
            current_job = next_job
            current_job.status = "Running"
            print(f"\n[PBS] Running job {current_job.job_id} (prio {current_job.priority}): {current_job.command}")
        while current_job.process.poll() is None:
            if scheduler_event.is_set():
                return
            time.sleep(0.1)
            with queue_lock:
                if pbs_queue and pbs_queue[0][0] < current_job.priority:
                    break
        with queue_lock:
            if current_job.process.poll() is not None:
                current_job.finish_time = time.time()
                print(f"[PBS] Job {current_job.job_id} completed")
                if current_job.job_id in jobs:
                    jobs[current_job.job_id].status = "Done"
                deallocate_pages(current_job.process.pid) # Deallocate memory after completion
                current_job = None

def scheduler(args):
    global current_scheduler, scheduler_thread, quantum, scheduler_event
    if len(args) < 2:
        print(f"Current scheduler: {current_scheduler}")
        return
    algorithm = args[1].lower()
    if algorithm not in SCHEDULERS:
        print(f"Invalid scheduler. Available: {', '.join(SCHEDULERS.keys())}")
        return
    if scheduler_thread is not None:
        scheduler_event.set()
        scheduler_thread.join()
        scheduler_event.clear()
    current_scheduler = algorithm
    if algorithm == 'rr':
        try:
            quantum = float(args[2]) if len(args) > 2 else 1
        except ValueError:
            print("Invalid quantum. Using default 1s")
            quantum = 1
        print(f"Using Round-Robin scheduler with {quantum}s quantum")
        with queue_lock:
            rr_queue.clear()
            rr_queue.extend([job for job in jobs.values() if job.status == "Running"])
        scheduler_thread = threading.Thread(target=round_robin_scheduler, daemon=True)
    else:
        print("Using Priority-Based scheduler")
        with queue_lock:
            pbs_queue.clear()
            for job in jobs.values():
                if job.status == "Running":
                    heapq.heappush(pbs_queue, (job.priority, job.arrival_time, job))
        scheduler_thread = threading.Thread(target=priority_scheduler, daemon=True)
    scheduler_thread.start()

def addjob(args):
    global job_counter
    try:
        priority = int(args[1])
        memory_pages = int(args[2]) # New: pages requested by the job
        command = args[3:]

        # Allocate memory for the job
        allocated = allocate_pages(job_counter, memory_pages)
        if allocated < memory_pages:
            print(f"Warning: Only allocated {allocated} out of {memory_pages} requested pages for job. Not launching process.")
            # If not enough memory, don't launch the process
            return

        process = subprocess.Popen(command, preexec_fn=os.setpgrp)
        new_job = Job(job_counter, process, " ".join(command), priority, memory_pages)
        jobs[job_counter] = new_job
        with queue_lock:
            if current_scheduler == 'rr':
                rr_queue.append(new_job)
            elif current_scheduler == 'pbs':
                heapq.heappush(pbs_queue, (priority, new_job.arrival_time, new_job))
        print(f"[{job_counter}] {process.pid} (prio {priority}, {memory_pages} pages) {new_job.command}")
        job_counter += 1
    except (IndexError, ValueError):
        print("Usage: addjob <priority> <memory_pages> <command>")

def metrics_cmd(args):
    print(f"{'JobID':<6} {'Command':<20} {'Priority':<8} {'Arrival':<10} {'Start':<10} {'Finish':<10} {'Waiting':<10} {'Turnaround':<12} {'Response':<10}")
    for jid, job in jobs.items():
        print(f"{jid:<6} {job.command:<20} {job.priority:<8} "
              f"{round(job.arrival_time,2):<10} "
              f"{round(job.start_time,2) if job.start_time else '-':<10} "
              f"{round(job.finish_time,2) if job.finish_time else '-':<10} "
              f"{job.waiting_time() if job.waiting_time() is not None else '-':<10} "
              f"{job.turnaround_time() if job.turnaround_time() is not None else '-':<12} "
              f"{job.response_time() if job.response_time() is not None else '-':<10}")

# ----------------- Built-in Dispatcher ----------------- #
builtins = {
    "cd": cd, "pwd": pwd, "echo": echo, "clear": clear, "ls": ls,
    "cat": cat, "mkdir": mkdir, "rmdir": rmdir, "rm": rm, "touch": touch,
    "kill": kill_proc, "exit": exit_shell, "jobs": jobs_cmd,
    "fg": fg, "bg": bg, "scheduler": scheduler, "addjob": addjob, "metrics": metrics_cmd,
    "meminfo": meminfo, "accesspage": accesspage, "run_producer_consumer": run_producer_consumer
}

# ----------------- Execute Commands ----------------- #
def execute_command(cmd_tokens, background):
    global job_counter
    if not cmd_tokens:
        return
    # Handle output redirection
    if '>' in cmd_tokens or '>>' in cmd_tokens:
        redirect_type = '>>' if '>>' in cmd_tokens else '>'
        try:
            idx = cmd_tokens.index(redirect_type)
            command_part = cmd_tokens[:idx]
            filename = cmd_tokens[idx + 1]
            mode = 'a' if redirect_type == '>>' else 'w'
            if command_part[0] == 'echo':
                with open(filename, mode) as f:
                    f.write(" ".join(command_part[1:]) + "\n")
                return
            else:
                with open(filename, mode) as f:
                    process = subprocess.Popen(command_part, stdout=f, preexec_fn=os.setpgrp)
                    if background:
                        # For simplicity, memory allocation for redirected commands is not managed here
                        # as they often short-lived or not meant to simulate long-running processes with memory
                        jobs[job_counter] = Job(job_counter, process, " ".join(command_part))
                        print(f"[{job_counter}] {process.pid}")
                        job_counter += 1
                    else:
                        process.wait()
                return
        except (IndexError, ValueError):
            print("Syntax error in redirection")
            return
    try:
        if cmd_tokens[0] in builtins:
            # Special handling for addjob to allow it to allocate memory
            if cmd_tokens[0] == "addjob":
                builtins[cmd_tokens[0]](cmd_tokens)
            else:
                builtins[cmd_tokens[0]](cmd_tokens)
        else:
            # For external commands, we can simulate a default memory allocation
            # For simplicity, let's assume they take 1 page by default if not specified by addjob
            num_pages_for_external = 1 # Default pages for external commands
            allocated = allocate_pages(os.getpid(), num_pages_for_external) # Using current shell PID for simplicity
            if allocated < num_pages_for_external:
                print(f"Warning: Not enough memory for external command '{cmd_tokens[0]}'.")
                return

            process = subprocess.Popen(cmd_tokens, preexec_fn=os.setpgrp)
            if background:
                jobs[job_counter] = Job(job_counter, process, " ".join(cmd_tokens), memory_pages=num_pages_for_external)
                print(f"[{job_counter}] {process.pid}")
                job_counter += 1
            else:
                process.wait()
                deallocate_pages(process.pid) # Deallocate memory after external command finishes
    except FileNotFoundError:
        print(f"{cmd_tokens[0]}: command not found")
    except Exception as e:
        print(f"Error executing command: {e}")

# ----------------- Main Loop ----------------- #
def shell_loop():
    while True:
        # Clean up completed jobs and their memory
        pids_to_deallocate = []
        for jid, job in list(jobs.items()): # Use list() to allow modification during iteration
            if job.process.poll() is not None and job.status != "Done":
                print(f"Job [{jid}] {job.command} has finished. Deallocating memory.")
                jobs[jid].status = "Done" # Mark as done to avoid repeated deallocation attempts
                pids_to_deallocate.append(job.process.pid)
        for pid in pids_to_deallocate:
            deallocate_pages(pid)


        try:
            cmd = input("myshell> ")
            if not cmd.strip():
                continue
            tokens = cmd.split()
            background = False
            if tokens[-1] == '&':
                background = True
                tokens = tokens[:-1]
            execute_command(tokens, background)
        except KeyboardInterrupt:
            print()
        except EOFError:
            print("\nExiting shell.")
            # Ensure all scheduler threads are stopped before exiting
            if scheduler_thread is not None:
                scheduler_event.set()
                scheduler_thread.join()
            os._exit(0) # Use os._exit for immediate termination

if __name__ == "__main__":
    shell_loop()
